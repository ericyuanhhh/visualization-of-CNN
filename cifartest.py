# -*- coding: utf-8 -*-
"""
Created on Sat Jul 22 23:04:41 2017

@author: eric yuan
"""

from __future__ import print_function

import numpy as np
np.random.seed(1337)  # for reproducibility
from keras.datasets import cifar10
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, Dropout,MaxPooling2D, Flatten
from keras.optimizers import Adam
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
batch_size = 32
num_classes = 10
epochs = 2
data_augmentation = True
model = Sequential()
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

#data pre-processing

X_train =X_train.reshape(-1,3,32,32)/255.
X_test = X_test.reshape(-1,3,32,32)/255.
y_train = np_utils.to_categorical(y_train,num_classes =10)
y_test = np_utils.to_categorical(y_test,num_classes=10)

img_rows, img_cols = 32, 32

if K.image_data_format() == 'channels_first':
    X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)
    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)
    input_shape = (None,3, img_rows, img_cols)
else:
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)
    input_shape = (None,img_rows, img_cols, 3)

# the first convolutional layer
model.add(Convolution2D(
        batch_input_shape = input_shape,
        filters=32,
        kernel_size=3,
        strides=1,
        padding='same',     # Padding method
))

model.add(Activation('relu'))
# Pooling layer 1 (max pooling) output shape (32, 14, 14)
model.add(MaxPooling2D(
    pool_size=2,
    strides=2,
    padding='same',    # Padding method
))


# Conv layer 2 output shape (64, 14, 14)
model.add(Convolution2D(64, 5, strides=1, padding='same', 
          ))
model.add(Activation('relu'))

# Pooling layer 2 (max pooling) output shape (32, 14, 14)
model.add(MaxPooling2D(
    pool_size=2,
    strides=2,
    padding='same',    # Padding method
))

#conv layer 3 
model.add(Convolution2D(32, 3, strides=1, padding='same', 
          ))
model.add(Activation('relu'))

# Pooling layer 3 (max pooling) output shape (32, 14, 14)
model.add(MaxPooling2D(
    pool_size=2,
    strides=2,
    padding='same',    # Padding method
))

#conv layer 4
model.add(Convolution2D(64, 3, strides=1, padding='same', 
          ))
model.add(Activation('relu'))
# Pooling layer 4 (max pooling) output shape (32, 14, 14)
model.add(MaxPooling2D(
    pool_size=2,
    strides=2,
    padding='same',    # Padding method
))

# Conv layer 5 output shape (64, 14, 14)
model.add(Convolution2D(64, 3, strides=1, padding='same', 
          ))
model.add(Activation('relu'))
# Pooling layer 5 (max pooling) output shape (32, 14, 14)
model.add(MaxPooling2D(
    pool_size=2,
    strides=2,
    padding='same',    # Padding method
))

model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dense(128))
model.add(Activation('relu'))
# Fully connected layer 2 to shape (10) for 10 classes
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax', name='preds'))


# Another way to define your optimizer
adam = Adam(lr=1e-4)

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
acc =[]
if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(X_test, y_test),
              shuffle=True)
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(X_train)

    # Fit the model on the batches generated by datagen.flow().
    history= model.fit_generator(datagen.flow(X_train, y_train,
                                     batch_size=batch_size),
                        steps_per_epoch=X_train.shape[0] // batch_size,
                        epochs=epochs,
                        validation_data=(X_test, y_test))
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
print('\nTesting ------------')
# Evaluate the model with the metrics we defined earlier
loss , accuracy = model.evaluate(X_test, y_test,batch_size=64,)

print('\ntest loss: ', loss)
print('\ntest accuracy: ', accuracy)
print(history.history.keys())
# summarize history for accuracy

model.save('my_cifar10_5.h5')
  
    












